Предистория:
Относительно экспериментов об ограничении действий (guidance_experiment) и нормировании наград (bounding_experiment) поменялись гиперпараметры.
При изменении системы наград получались менее стабильные результаты, но с улучшением качества в пиках.
1. Размер сети внутри Q-функции был увеличен в несколько раз. Это могло улучшить сходимость, потому что функция наград стала сложнее и маленькая сеть не могла хорошо ее аппроксимировать.
2. Количество карточек, которое видет агент тоже было увеличено. Это помогает агенту в начале обучения, когда он мог купить слишком много карточек.