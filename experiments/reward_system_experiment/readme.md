Вводная:
Относительно экспериментов об ограничении действий (guidance_experiment) и нормировании наград (bounding_experiment) поменялись гиперпараметры.
1. Размер сети внутри Q-функции был увеличен в несколько раз. Это могло улучшить сходимость, потому что функция наград стала сложнее и маленькая сеть не могла хорошо ее аппроксимировать.
2. Количество карточек, которое видит агент тоже было увеличено. Это помогает агенту в начале обучения, когда он мог купить слишком много карточек.

Система наград сильно повлияла на результаты обучения.

| Experiment  |   Win |   Lose |        % |
|:------------|------:|-------:|---------:|
| Default     |   352 |    948 | 0.270769 |
| Potential   |   587 |    713 | 0.451538 |

Параметер p-value равный 1.2479e-21 сигнализирует о большой статистической значимости.